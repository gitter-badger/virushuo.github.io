<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programming on </title>
    <link>/tags/programming/</link>
    <description>Recent content in Programming on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 27 Dec 2009 18:12:00 +0800</lastBuildDate>
    
	<atom:link href="/tags/programming/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>使用php,js来对内容做rsa加密</title>
      <link>/posts/phpjsrsa/</link>
      <pubDate>Sun, 27 Dec 2009 18:12:00 +0800</pubDate>
      
      <guid>/posts/phpjsrsa/</guid>
      <description>http://code.google.com/p/phpjsrsa/
这是一个用于文本加密的库，主要用于http协议下的防窃听。一般来说，如果应用https协议可以有效的避免窃听。但有几种情况必须考虑。
(1) 主机同时有https和http协议，部分用户通过https协议访问，获得了保护。但也有用户通过http访问，这部分用户会遭到窃听。除非关闭http请求，全面转向https。 (2) 主机并没有https支持。
很多情况下，我们需要保证主机安全，最好的办法是将其混入数字森林中。即：这台主机输出的内容没有人能看得懂的，只由无意义的代码和数字组成。用户浏览这台主机，不会触发任何关键词扫描。甚至该主机连https协议都不使用，更凸显其低调本色。
换言之，一个网站如果把自己的内容都变成字母和数字的组合，且不使用https协议，那么他就是数字森林中的一片树叶，丝毫不引人注意。
我们的目标应该是传输过程中不引人注意，并非绝对的不可破解的安全。
因此这个库的工作流程是：
 php对&amp;rdquo;内容&amp;rdquo;做rsa加密-&amp;gt;将加密结果输出到页面上。 用户浏览页面，html代码中的&amp;rdquo;内容&amp;rdquo;被加密成数字形态。私钥可以直接输出在页面代码中，也可由用户输入一次，保存在cookie中。使用cookie会降低密钥泄露的危险，更加有效。 通过javascript在用户浏览器上将这些数字解密为内容。 通过javascript dom来把内容写回到页面上。用户即可浏览。  利用javascript解密，可以把运算负担分散到客户端上。窃听者如要窃听每一个页面的内容，则必须要 1 获得密钥 2 用密钥解密内容
在已知密钥情况下，如客户端的每个页面运算负担为 1 ，页面数量n ，那么窃听者获得密钥之后的运算负担为 1*n。
为了运算效率，使用小质数作为rsa的p,q，理论上窃听者可以通过因数分解算出密钥，其运算负载为k，注意k 远远大于1。
如果每个站点使用不同的密钥，共计m个站点，窃听者的运算负担为 m*k+1*n，且负载集中。
而，如果采用双向可逆加密方法，在得知算法的情况下，窃听者运算负载极小。如果在通过变换算法来增加难度，又无法做到通用，给用户正常浏览造成困难。使用rsa方法，算法是标准的，用户使用成本很低，窃听成本很高。
在项目代码中，我已经实现了这一目标。但仍然有效率问题。
目前问题：
1 在没有bcmath和gnumath函数的php主机上，php加密内容的运算效率很低。和bcmath差距几十倍。好在大部分情况下，主机都是有bcmath函数的。这个问题不严重。 2 JS的bigint运算效率很低，主要是powmod的效率低，而这是rsa解密最频繁的操作。
希望有兴趣的朋友加入这个项目。效率问题解决后，还需要port在一系列常用软件上。比如dabr或twitese等。
另外，需要的质数可以在 http://www.prime-numbers.org 找。
update: 使用了 http://www-cs-students.stanford.edu/~tjw/jsbn/ 的大数运算库，效率提高很多。
2010/01/01 Ariagle has released a http://blog.lolily.com/wordpress-plugin-wp-no-keyword.html&#39;&amp;gt;wordpress plugin , It encodes keyword into digitals by phpjsrsa :)</description>
    </item>
    
    <item>
      <title>谈谈《程序员修炼之道》</title>
      <link>/posts/the_pragmatic_programmer/</link>
      <pubDate>Sat, 08 Aug 2009 00:08:00 +0800</pubDate>
      
      <guid>/posts/the_pragmatic_programmer/</guid>
      <description>学过高中物理的人，应该会记得，原子中的电子获得能量之后，将发生能级跃迁，到达更高的能量状态。其实任何工种都是一样的，要跳出自己的水平，到达更高的级别，不是件容易的事，这个跳跃过程总需要一些东西的辅助。诚然，如果要成为一个好人，那么只要做好在幼儿园中学到的一切就足够。如果要成为一个好程序员，所需要的道理也不太多，只不过，当水平不够的时候，永远不能认识到那些朴素道理的重要。而当水平达到的时候，这些道理自然会明白。所以一本帮助程序员进阶的书，很容易落到低手觉得是废话，高手也觉得是废话的悲惨境地。
很多年以前，有人和我推荐过这本《程序员修炼之道》，甚至专门买了一本送到我家。而当年的我，不知道是由于无知，自负，浮躁，或是其他，只草草翻了一下，就下了个&amp;rdquo;烂书&amp;rdquo;的定义，扔在书架一角。后来有朋友在我书架上发现，如获至宝，说已经买不到了。我当然乐得送了人情。在我心目中，最好的入门书永远是《代码大全》，那也是对我影响最深的一部书。
过了很多年之后，再来谈这本书，发现很多人的评价比我高的多，自知不妙，赶快找来重读，这才知道错过了什么。在一个滥俗的译名之下，在一个看起来不知所云的目录之后，在一些读起来拗口的句子之中，隐藏的竟然是相当伟大的思想，朴素而真挚，简单而有效。这时候我突然明白，这是一本不逊于《代码大全》的伟大著作，后者一直被我誉为&amp;rdquo;新手圣经&amp;rdquo;。
经验这个东西，往往并不能告诉我们什么一定对，但是可以告诉我们什么一定不对。这本书完全是经验凝成，没有大道理，没有新观念。这些朴素的道理就是创造一个合格软件和作一个好程序员所必须了解的。比如 &amp;ldquo;提示44 不要靠巧合编程&amp;rdquo;，这句话表达的意思是&amp;rdquo;不要预设立场&amp;rdquo;。听起来简单，但是只要随手翻翻你最新写过的一段程序，通常都会发现代码中做了大量的&amp;rdquo;假设&amp;rdquo;。书中用一道习题，假设了用户使用命令行环境，假设用户懂英语&amp;hellip;.都可能导致问题。怕了吧？幸好还有&amp;rdquo;提示30 你不可能写出完美的软件&amp;rdquo;，这可不是帮你开脱责任，而是在讲如何控制需求，这正是能顺利完成一个项目的根本前提，可惜事实上往往到了项目失败的时候，人们才想起来需求出了问题。
这本书涉猎的范围相当广，如何设计架构，如何思考问题，如何测试，如何编码，如何处理文档&amp;hellip;如果细心琢磨，构建软件的所有主干和细微枝节都有所涉及。和很多人的看法不同，我不认为这是一本可以轻松读完的书。一方面，这本书涉及的内容太多，虽然已经尽量讲述，但所有话题都可以继续引申出无限的内容，如果用心，还可以配合附录中所提到的各种论文和资源继续学习。习题也需要仔细思考。这绝不是一本小说。另一方面，作者用了大量的隐喻，导致读起来有一定难度。开始我认为是翻译质量有问题，不过慢慢发现美国的读者读起来也未必容易。原因还是涉及到的范围过大。我特意模仿这种风格写了本文的第一段。虽然是中文，读起来也不容易吧。
相信以上的两点会阻挡一部分人阅读这本书。我也算是曾经受阻的人之一。 不过，好书并不会随着时间的推移和平台变化而消亡，好书只会成为经典。无论是《人月神话》，还是《代码大全》，都在时间的长河中沉淀下来，传颂至今。这本书，虽然历史只有10年，不过现在再来翻看，不仅毫不落伍，甚至感觉穿透了时间，看到了这些年中不少自己犯过的错误，我相信这也是一本能经的起时间沉淀的书，只不过需要多点耐心。因此，我郑重的写下这篇书评，希望再能读到这本书的人多一点耐心，越过语言的障碍，直入本质，直至跃向更高级别。这个希望，不仅仅是对新手说的，其实也包括我自己。如本书开头所说：注重实效的程序员应该不断学习。我们都应该不断的学习下去。</description>
    </item>
    
    <item>
      <title>技术改变世界以及减速慢行</title>
      <link>/posts/dic_comment/</link>
      <pubDate>Tue, 03 Jun 2008 11:06:00 +0800</pubDate>
      
      <guid>/posts/dic_comment/</guid>
      <description>《梦断代码 （Dream In Code）》 韩磊 译 ，有幸先睹，颇有所感，做书评一篇。
拿着这本刚刚出炉的《梦断代码(Dream In Code)》，似乎让我回到了2004年夏天，那时候非典的恐惧刚刚散去。北京西二环边上的一座高档办公楼的施工重新开始了，这座大厦的9层，数百个工人正在各自忙碌，以期能弥补非典期间项目耽误掉的时间。看着杂乱又有序的工作场面，正在负责大厦中央集成软件系统的我不禁暗自赞叹——别说几百个人，就算是给我1/10于这个数字的程序员，恐怕都足够让我把这个软件项目搞砸了。
软件项目的管理者总是无比艳羡传统行业，无论是工业的流水线还是建筑的工地，只需要增加人手就可以赶上进度，这样的情形在软件行业中完全就是神话，“人月神话”。恰好也是在2004年，不知道是不是还有很多人和我一样被非典之后建筑项目赶工场面所震撼，软件行业的人们开始无比的期盼跟传统行业学习。这一年的年末，美国建筑师亚历山大所著的《建筑的永恒之道》一书突然热卖。这本建筑专业书荣登了各大网站书店的IT热书排行榜。可惜软件绝非建筑，软件是人类历史上唯一的完全在大脑中靠智慧完成的工作。而我们还没有找到管理一个只在大脑中完成的项目的方法。
硅谷有名言曰：“技术改变世界”。计算机发展了几十年，就快速的渗透了世界的各个角落。比起以往的工业革命，这次信息革命确实以无法想像的速度飞速的改变着世界。软件行业好像一个拥有了姚明的身高，爱因斯坦的智慧，但是却没有任何社会经验的巨人一般，威力巨大，但又完全无法琢磨。
相对于手上这本《梦断代码》描述的Chandler项目，我职业生涯中所看到的种种软件项目的失败都算不了什么。似乎所有项目都能找到一个失败的理由，或者说市场压力太大，工期不足，或者说资金不够，或者说程序员团队经验不足。在《梦断代码》中，这些问题都不存在，那么，是否可以让一个成功的产品横空出世呢？这个两打优秀程序员组成的团队，用他们失败的经历明确的告诉了我们：不能。
在软件行业不长的历史中，我们随处可见失败者。无论是小公司草草上马的小项目，还是大公司经过百般论证的大项目，似乎都难逃焦油坑，一个个落得了个进退不能的下场。当然，另一方面，成功者也并不罕见，Linux经过了10多年，越发成熟和优秀。微软总在进行着“这个星球上最大的软件项目”，虽然反对者百般嘲笑其软件质量低下，也并不能阻止这家公司依靠软件成为行业的垄断者。《梦断代码》中提到，Linus说，从小处着手的项目更容易获得成功。果然，从大处入手的Chandler项目深陷泥潭。但是，为什么同样从大处入手的Outlook甚至整套Office成功了呢？悲观的看来，我们大概永远也无法知道什么是对的。
幸运也不幸，人类文明也是一部试错史。人们很难预知如何做才对，所以只好一次又一次的撞墙，自省，撞墙，自省，撞的头破血流的时候，总算得到了一些经验。经验这个东西，确实也无法告诉我们应该如何做，但是至少可以告诉我们如何做肯定是错的。讲软件项目失败案例的书足够排满一个书架。但是，《梦断代码》仍然是颇具现实意义的一本，一方面是失败的故事有多少都算不得多，另一方面是，和Chandler项目这样，看起来“万事具备”，结果却“只欠成功”的项目样本确实不多。这足以让软件从业者们明白，这个行业的问题比想像的还严重。
失败的教训总是由血泪凝成。Chandler以3年时间，两打程序员，无数的资金，铸就了一面警示牌。而Scott所著的《梦断代码》，则把这块警示牌挂在了软件世界的高速公路上最显眼的位置上。上面赫然写着“前方危险，减速慢行”。
我敢打赌，这条危险的道路上绝对不会缺少新的墓碑，年轻的程序员和项目经理们仍然狂妄而自负的横冲过来。不过我至少希望更多的人可以读读这本书，然后在碰到类似的情况的时候，提醒自己减速慢行。</description>
    </item>
    
    <item>
      <title>web2.0推动的互联网基础技术变革</title>
      <link>/posts/web2_infrastructure/</link>
      <pubDate>Sun, 26 Aug 2007 21:08:00 +0800</pubDate>
      
      <guid>/posts/web2_infrastructure/</guid>
      <description>胡狼发来 http://tech.51cto.com/art/200703/42476.htm 给我看。
这篇文章最有趣的地方是说到了web2.0公司更加善于不重复发明轮子。而在这个领域最成功的案例是amazon的S3和EC2服务。amazon和google是我们一直关注的两个公司，某种程度上他们代表了互联网的未来和方向性。这事情值得讨论一番。
行业的变化
其实整个行业的变化早就在不经意间发生，就算是看起来最稳定的领域。很多人认为unix世界是稳定的，windows世界是快速变化的，其实正好相反。unix世界每分钟都在变化。同样，在互联网最基础的层面，变化也在时刻发生。
互联网基础的基础，是带宽和服务器。95年的时候，建网站一般是自己拉一根专线到办公室，接在“服务器”上。其实直到现在，很多企业还在自建机房。比如我过去工作过的税务报社，比如过去我们的兄弟单位经济日报社，都是自己的机房。这些都是有钱，且更在乎信息安全的单位，算特例。
IDC的产生我认为是通过集中管理带宽来产生效益的方法。机器放在IDC托管，比自己建机房便宜，投入小，而IDC还能赚到钱。这是规模效应产生的利润。 IDC满足了99%的网站。除了刚才说的那种国企和机关。
更特殊的是google和amazon。他们对带宽的要求，对成本的控制，整体的规模要求，已经没有idc能够承担了。于是也只好自己建机房。不仅建造机房，为了支撑自己的服务，他们还要开发很强的软件用于管理。比如google的gfs和bigtable用于存储(在修改过得linux文件系统上面的一层)，gws用于负载平衡和web 服务。按照机器数量和带宽数量来衡量的话，google是世界上最大的IDC了。
amazon做得更有趣一些。除了自己建立了大型的分布的机房，完成了各种软件应用，还把空闲的资源分割开出租。这就是AWS(aws.amazon.com)。AWS应用最成功的服务是存储服务S3（Simple Storage Service）和运算服务EC2(Amazon Elastic Compute Cloud)。这东西价格便宜到不可想象的地步，核算一下成本就会发现，任何规模小点的公司自己建立存储，都不如采用S3划算和安全，小公司的带宽投入价格甚至会超过购买同样带宽的S3。而且不需要投入开发人员，直接用就行了。
这仍然是“规模效应产生利润”。前面说了，IDC完成了“带宽”这个基础的规模效应。同样拥有大量带宽的amazon，同时又具有强大的软件系统，这种基础将可以进行规模效应的产品向上推了一层。不再是硬的带宽和服务器，而是软的存储技术。这就是我称之为互联网基础技术的东西。
amazon在规模效应上远高于IDC。首先，带宽采购量更大，价格更便宜。其次，服务器和存储设备统一采购，价格更便宜。然后，自己开发的软件进行统一管理，大大降低管理成本。后面两个层次是IDC所不具备的。这是软件的力量。
新一代网站与基础技术渴望
与其叫他们web 2.0，我更愿意叫他们新一代网站。他们有几个特点：
1 目标更收缩集中。相册能作成一个大网站(flickr.com)，书签能作成一个大网站(delicio.us)，甚至唧唧歪歪的留言也可以(twitter.com)&amp;hellip;&amp;hellip;任何一个独立的领域都有可能成为一个单独的服务被拿出来。 2 公司规模更小。比起传统的互联网企业，新一代的网站工作人员数量变得很少。不再需要那么多编辑，不再需要那么多销售&amp;hellip; 3 传统行业大量涌入。大量的传统行业者利用现有资源，突击互联网。他们的特点是行业资源强，盈利能力强，技术能力差。 4 网站的架构变得更加简单。因为目标收缩集中了，网站的架构也就简单了。 5 内容量快速增加。用户创造内容并非空话，新的网站中内容增加的速度远远高于过去。 6 网站之间因为api的缘故，互相连通更加容易。
目标更加集中，导致架构简单，而强大的互动特性导致了用户贡献内容，使得公司规模更小，更追求低成本。应用层技术要求下降，基础技术要求提高。比如：应用层就一个bbs，但是这个bbs有了5000万个帖子。这时候需要的是存储技术，而不是简单改改bbs程序就能解决的。过去复杂的网站架构已经变成了简单应用+高稳定+大负载+高容量的简单架构。架构简单了，但是难度反而提高了。作一套复杂的论坛程序还是简单的，但是做一个高稳定+大负载+高容量的存储服务，就很困难了。
我们急切的需要抹平这个技术鸿沟。现在做一个网站有很多可选组件了。比如，论坛有的是开源程序，图片可以通过flickr api放在flickr，需要地图可以用google map api 或是51ditu api，至于facebook api提供的功能就更多了。有了这些东西，做出一个想象中的应用就变得简单多了。而，在应用之下的技术基础服务，却仍然不够丰富。只有amazon算是前瞻的走出了一步。
我们做咨询的过程中，发现大量网站因为基础技术问题而导致不稳定。这问题在一年后会变的更为突出。这一年所累积的内容和数据会令很多网站不强大的技术基础彻底崩溃。
基础技术服务
以我看来，基础技术分为几类，存储算一方面，搜索算一方面，负载算一方面。tiny早有名言说：每一个网站都应该有反向代理缓存，每一个网站都应该有搜索技术。至于存储，不是每个网站都需要，至少也是大部分网站都需要。
所有公司都习惯租办公室而不是买楼，租比买好。按照新一代网站的实际情况和务实性，他们也应该更喜欢租用这些服务。amazon S3在美国的火爆充分证明了这一点。我们开始尝试的对外出租搜索技术的目前看来情况也不错。
而最好的服务模式，我觉得是以机房为单位，在一个机房内网范围内提供基础技术服务，这样速度有保障，且不产生外网流量，非常理想。一个网站可以用这些东西拼拼搭搭，完成自己的服务，把精力放在自己特定的业务和客户上，他们不可能因为技术垄断而产生竞争壁垒，也就没必要为了技术投入太多资源。
在还没有amazon的中国，这个服务恐怕需要几个角色合作完成了。
事实上，互联网的特征是，一个东西一旦普及到一定程度，就应该免费。然后通过在此之上的增值服务赚钱。这也是必然的趋势。
胡狼说了个很好的比喻:&amp;ldquo;带宽和合适的软件的结合，就好像intel+ms的联盟一样&amp;rdquo;。窃以为然。</description>
    </item>
    
    <item>
      <title>google发布bigtable论文</title>
      <link>/posts/googlebigtable/</link>
      <pubDate>Mon, 25 Sep 2006 15:09:00 +0800</pubDate>
      
      <guid>/posts/googlebigtable/</guid>
      <description>8个人开发了2年半，现在bigtable的神秘面纱终于揭开了。google发布了一篇相当详细的论文“Bigtable: A Distributed Storage System for Structured Data ”
这篇论文内容空前详细，包括bigtable的目的，数据模型，一些实例api调用的代码，性能参数，还有和其他相关产品的比较。
如标题所述 ，bigtable是一个用来存储结构数据的分布式存储系统。与平时常用的数据库不同，bigtable并非一个支持sql语言的关系数据库，而是map方式的，列导向的数据库（一列数据连续存储）。bigtable为读进行了优化，对数据库的读取访问远远大于写入是互联网服务的重要特点。bigtable的时间特性也颇为引人注目，bigtable中数据都带有timestamp字段，可以保存不同时间的多个版本。
论文中提到，google已经有6个服务已经运行于bigtable上了。分别是：Google Analytics,Google Earth,Personalized Search,Google Finance, Orkut,Writely。这里面我觉得最值得注意的是Writely和Analytics，这两个都是google收购来的服务，通过一段时间的改造，已经重组了其架构，使他们成为可以承担海量负荷的大型服务。这似乎也标志了google对于Writely的重视。
特别值得注意的是，这6个服务都是恰好带有明显时间特性的服务，借助bigtable的时间特性，可谓如虎添翼。最近Google Earth也增加了时间的标签。将来，bigtable必将用于更多的地方，事实上，时间标签对于web服务是相当重要的特征，但由于数据量太大，保存困难，限制了很多应用的发展，bigtable应用于wiki或是archive.org之类的服务的时候，必将势如破竹。
以前我们分析过，google通过收购和内部创业等方式获得新型服务，然后通过强大的基础技术改造这些服务，使其成为高可用性，高负荷高稳定性的服务。这或许就是google未来的发展方向。google通过一系列的包装，使分布式数据库这样复杂的东西可以被简单的api调用，这无疑将大大提高google内部各小组的开发能力。
ps:感谢youfeng及时提供这个消息。</description>
    </item>
    
  </channel>
</rss>